인공지능(人工知能)은 철학적으로 인간성이나 지성을 갖춘 존재, 혹은 시스템에 의해 만들어진 지능, 즉 인공적인 지능을 뜻한다. 일반적으로 범용 컴퓨터에 적용한다고 가정한다. 이 용어는 또한 그와 같은 지능을 만들 수 있는 방법론이나 실현 가능성 등을 연구하는 과학 분야를 지칭하기도 한다.인공지능이 무엇이냐는 질문은 두 가지로 나눌 수 있다. "인공이란 무엇인가?"와 "지능이란 무엇인가?"이다. 첫째 질문은 꽤 답하기 쉽지만, 무엇을 인공적으로 만들 수 있는가 하는 질문을 낳는다. (여기서 만든다는 것은 고전적인 연산 시스템과 같은 특정 형태의 시스템 하에서 이루어 질 것, 인공적 제조 공정이 존재할 것, 그리고 인간 지능의 한계라는 테두리 안에서 이루어진다.)둘째 질문에 대답하기는 훨씬 어려운데, 이는의식이나자아혹은심리(무의식을 포함해서) 등이 무엇인가, 그리고 우리가 연구할 수 있는 유일한 종류의지능인 인간의 지능은 어떠한 요소로 구성되어 있는가 하는 문제를 제기하기 때문이다. 인간의 지능적인 행동을 연구하거나 이해하는 것은 무척이나 어렵고 복잡한 작업이다. 기존의 모델과 다른 각도에서 접근하고 있는 동물과 인공지능의 관계에 대한 연구는 그 타당성을 널리 인정받고 있다.개념이 뚜렷한 형태의 일부 인공지능은 아래에 설명되어 있다. 인공지능의 주제별 분류, 역사, 그리고 주제별 장단점 및 응용 사례에 대해서 기술되어 있다. 끝으로 인공지능이 등장하는 소설 및 비소설 목록이 마련되어 있다.초기 인공지능 연구에 대한 대표적인 정의는 Dartmouth Conference에서존 매카시가 제안한 것으로 "기계를 인간 행동의 지식에서와 같이 행동하게 만드는 것" 이다. 그러나 이 정의는 강한 인공지능에 대한 고려를 하지 못한 것 같다. 인공지능의 또다른 정의는 인공적인 장치들이 가지는 지능이다. 대부분 정의들이 인간처럼 사고하는 시스템, 인간처럼 행동하는 시스템, 이성적으로 사고하는 시스템 그리고 이성적으로 행동하는 시스템이라는 4개의 분류로 분류된다.강한 인공지능은 어떤 문제를 실제로 사고하고 해결할 수 있는 컴퓨터 기반의 인공적인 지능을 만들어 내는 것에 관한 연구다. 즉, 인공지능의 강한 형태는, 지각력이 있고, 스스로를 인식하는 것이라고 말할 수 있다. 이론적으로 강한 인공지능에는 두 가지 형태가 있다.약한 인공지능은 어떤 문제를 실제로 사고하거나 해결할 수는 없는 컴퓨터 기반의 인공적인 지능을 만들어 내는 것에 관한 연구다. 그와 같은 시스템은 진짜 지능이나 지성을 갖추고 있지는 못하지만, 어떤 면에서 보면 지능적인 행동을 보일 것이다. 오늘날 이 분야의 연구는 주로 미리 정의된 규칙의 모음을 이용해서 지능을 흉내내는 컴퓨터 프로그램을 개발하는 것에 맞추어져 있다. 강한 인공지능 분야의 발전은 무척이나 미약했지만, 목표를 무엇에 두느냐에 따라 약한 인공지능 분야에서는 꽤 많은 발전이 이루어졌다고 볼 수 있다.John Searle나 Hubert Dreyfus와 같은 몇몇 철학자들은 몸이 아닌 기계에 인간의 지능이나 의식을 구현하는 작업의 실현 가능성에 대한 철학적 바탕을 두고 논쟁을 벌였다. Searls은,튜링 테스트의 통과 여부는 사람의 기준으로 볼 때 기계가 의식을 갖추었다는 판단의 필요 조건이 되지 못한다는중국어 방(Chinese Room)에 대한 논증으로 유명하다. Hubert Dreyfus는 그의 저서 "컴퓨터가 할 수 없는 것들: 인공적인 추론에 대한 비평"에서 의식이라는 것은 룰이나 논리 기반 시스템 또는 물리적인 형태를 가지고 있지 않은 시스템에서 찾을 수 없으나,신경망(neural network)이나 그 유사한 메커니즘을 이용하는 로보틱 시스템은 인공지능을 실현할 수 있는 가능성이 있다고 주장했다.다른 철학자들은 엇갈린 관점을 고수한다. 많은 사람들이 약한 인공지능 정도는 가능하다고 보지만, 또한 많은 사람들이 강한 인공지능을 지지하고 있는 것도 사실이다. Daniel C. Dennett은 그의 '의식에 대한 설명'에서 만일 마법의 불꽃이나 영혼이 없다면 인간은 기계에 불과하다며, 지능에 대해서만 인간이라는 기계가 다른 실현 가능한 모든 기계와 다르게 특별 취급을 받아야할 이유가 무엇인가 묻고 있다.어떤 철학자들은 우리가 약한 인공지능을 가능한 것으로 받아들인다면, 강한 인공지능 역시 받아들여야 한다고 주장한다. 지능은 (외견상) 보여지는 것이지, 진정한 무엇이 아니라는 약한 인공지능의 입장은 많은 비판을 받고 있다. 그러나 이에 반하는 손쉬운 예를 Simon Blackburn의 철학 입문서 "생각"에서 찾을 수 있다. Blackburn은 당신이 지능적으로 보이지만, 그 지능이 진정한 것인가에 대해서 말할 수 있는 방법이 없다고 지적한다. 그는 우리는 단지 믿음 또는 신념 위에서 그것을 다룰 뿐이라고 이야기한다.강한 인공지능을 지지하는 사람들은 인공지능에 반대하는 사람들의 논증이 결국은 아래와 같은 주장을 조합한 것이라고 주장한다.강한 인공지능을 뒷받침하는 논증(따라서 반대하는 사람은 이 논증을 논박해야한다)은 다음과 같다.로저 펜로즈를 포함한 몇몇 사람들은처치-튜링 명제의 적용이 가능하지 않다고 논박한다. 어떤 이들은 인간의 마음은 물리적인 속성을 뛰어넘는 무엇이 있다고 이야기한다. 로저 펜로즈의 주장은 우리의 우주 안에서고도연산(hypercomputation)이 가능하다는 논증에 바탕을 두고 있다. 양자역학과 뉴턴 역학에 따르면 이러한 고도연산은 가능하지 않지만, 특별한 우주 시간(space times)에서는 가능한 것으로 생각되기도 한다. 그러나 일반적으로는 우리의 우주는 그와 같은 고도연산이 가능할 정도로 꼬이지(convoluted) 않았다는 합의가 존재한다.상당수 인공지능 연구의 (본래) 목적은 심리학에 대한 실험적인 접근이었고, 언어 지능(linguistic intelligence)이 무엇인지를 밝혀내는 것이 주목표였다(튜링 테스트가 대표적인 예이다).언어 지능을 제외한 인공지능에 대한 시도들은로보틱스와 집합적 지식(?)을 포함한다. 이들은 환경에 대한 처리, 의사 결정을 일치시키는 것에 중심을 두며 어떻게 지능적 행동이 구성되는 것인가를 찾을 때,생물학과,정치과학으로부터 이끌어 낸다. 사회적 계획성과 인지성의 능력은 떨어지지만 인간과 유사한 유인원을 포함한, 복잡한 인식방법을 가진 동물뿐만 아니라 특히 곤충들(로봇들로 모방하기 쉬운)까지 포함한 동물학으로부터 인공지능 과학은 시작된다.인공지능 학자는 동물들은 인간들보다 모방하기 쉽다고 주장한다. 그러나 동물의 지능을 만족하는 계산 모델은 없다. 워랜 맥컬로치가 쓴 신경 행동에서 내재적 사고의 논리적 계산[1],앨런 튜링의 기계와 지능의 계산[2]그리고, J.C.R. 릭라이더의 인간과 컴퓨터의 공생[3]가 기계 지능의 개념에 관한 독창적인 논문들이다.존 루카스의 지성, 기계, 괴델[4]과 같은 논리학과 철학기반의 기계지능의 가능성을 부인한 초기 논문들도 있다.[5]인공지능 연구에 바탕을 둔 실질적인 작업이 결실을 거둠에 따라, 인공지능을 지지하는 사람들은 인공지능의 업적을 깎아내리기 위해 인공지능에 반대하는 사람들이 예전에는 '지능적'인 일이라고 주장하던 컴퓨터 체스나 음성인식 등과 같은 작업에 대해 말을 바꾸고 있다고 비난하였다. 그들은 이와 같이 연구 목표를 옮기는 작업은 '지능'을 '인간은 할 수 있지만, 기계는 할 수 없는 어떤 것'으로 정의하는 역할을 한다고 지적하였다.(E.T. Jaynes에 따르면)존 폰 노이만은 이미 이를 예측하였는데,1948년에 기계가 생각하는 것은 불가능하다는 강의를 듣고 다음과 같이 말하였다. "당신은 기계가 할 수 없는 어떤 것이 있다고 주장한다. 만일 당신이 그 기계가 할 수 없는 것이 무엇인지를 정확하게 이야기해준다면, 나는 언제든지 그 일을 수행할 수 있는 기계를 만들 수 있다." 했다.폰 노이만은 이미 그 전에 모든 처리절차(procedure)는 (범용)컴퓨터에 의해서 시뮬레이션 될 수 있다고 이야기함에 따라 쳐치-튜링 이론을 언급했다.1969년에 매카시와 헤이스는 그들의 논문 "인공지능 관점에서 바라본 철학적인 문제들"에서프레임 문제를 언급하였다.인공지능이란 분야를 처음 인식하기 시작한 것은 1943년 McCulloch 와 Pitts에 의해서 시작 되었으며 그 동기는 뇌에 있어서 뉴런의 기능과 작용에 대한 연구와명제 논리그리고 튜링테스트 연구였다. 뉴런으로 연결된 네트워크 학습의 개념이 필요함을 주장하였으며 인간의 사고 과정을 최초로 연결망을 통해 모델화 했다는 점에서 인공지능 역사상 의의가 매우 크다고 볼 수 있다.1956년 여름 다트마우스(Dartmouth) 대학의 캠퍼스에 '생각하는 기계', 즉 지적인 행동을 하는 컴퓨터에 관해 토론하기 위해서 많은 사람들이 모였으며(수학자,심리학자,전기기술자,대학의 연구자 기업가 등등) 이들은 디지털 계산기를 '생각하는 기계'로 만들 수 있다는 신념을 가지고 있었고 이 회의가 인공지능(artifitial intelligence&#160;: AI)이란 단어를 탄생시킨 계기가 되었다. 이 회의는 공식적으로 '다트마우스 하기 인공지능 연구계획'이라 불리며 그 뒤의 10년 간을 지배한 여러 연구성과를 냈다. 이것이 제1차 인공지능 연구의 시작이었다.인공지능은1959년에MITAI연구소를 설립한 매카시와 마빈 민스키,카네기멜론 대학교에 인공지능 연구소를 만든앨런 뉴웰과허버트 사이먼과 같은 개척자들에 의해 1950년도에 실험 학문으로 시작되었다. 이들 모두는 1956년에 매카시, 민스키,IBM의 나단 로체스터 와 클라우드 샤논에 의해 조직되어 열린, 이미 언급된 도르트문트 대학의 여름 AI 콘퍼런스에 참가하였다.역사적으로, 인공지능 연구는 두 개의 부류 -- 깔끔이(Neats)와 지저분이(Scruffies) -- 로 나눌 수 있다. 깔끔이는 우리가 전통적 혹은 기호적(symbolic) 인공지능 연구라고 부르는 분야로서, 일반적으로 추상적인 개념에 대한 기호적 연산과 전문가 시스템(expert systems)에 사용된 방법론을 가르친다. 이와 상반된 영역을 우리는 지저분이(Scruffies) 또는 연결주의자(connectionist)라 부르는데, 시스템을 구축하여 지능을 구현/진화시키려고 시도하고, 특정 임무를 완수하기 위해 규칙적인 디자인을 하기보다는 자동화된 프로세스에 의해서 지능을 향상시키는 방식이다. 가장 대표적인 예로신경망(neural network)이 있다. 이 두 가지 접근법은 인공지능 역사의 매우 초창기부터 함께 했다. 1960년대와 1970년대를 거치며 scruffy 접근법은 주목받지 못했지만, 1980년대 깔끔이 접근법의 한계가 명확해지면서 다시 주목 받게 되었다. 그러나 현재 두 가지 방식을 사용하는 그 어떤 최신의 인공지능 알고리즘도 확실한 한계가 있다는 것이 명확하다.특히 1980년대에 들어서 Back propagation (인공지능 학습방법: Training Method)가 소개되면서 많은 연구가 진행되었음에도, 신경망을 이용한 인공지능은 아직 초보단계이다. 인공신경망 (Artificial Neural Networks)을 이용한 많은 연구가 현재에도 진행되고 있지만, 몇 가지 장애로 인해서 실용화하기엔 아직도 먼 기술이다. 인공신경망을 이용한 인공지능이 어느 정도 실용화되기 위해선 우선 실효성 있는 학습방법 (Training Methods)이 필요하다. Back propagation을 이용한 학습방법이 제안되어 연구되고 있지만, 완전한 학습을 이룰 수 없을 뿐만 아니라, 학습에 사용되는 data들이 서로 orthonormal해야 하는 조건 때문에 항상 불완전한 학습으로 끝나기 쉽다. (Converge to Local Mimimum, not to the optimal minimum: 지역최적해에 머뭄. 즉, 눈먼 장님이 가장 낮은 저지대를 찾는 경우 각 현재 지점에서 아래로 내려가려는 성질이 있는데 이때 눈먼 봉사이므로 특정 지점의 저지대에 도달한 경우, 그 지점에선 어디로 가거나 위로 올라가는 것만 있으므로 앞에 설명한 성질에 의해 바로 전에 찾은 저지대 남으려 하는 성질이 있다는 것을 의미함). 이러한 단점들을 보완하기 위해서 Fuzzy Logic, Neurofuzzy (Neural fuzzy logic) and Genetic Algorithms등을 이용한 학습방법이 연구되고 있으나 전망이 밝지만은 않은 상태이다.미국의DARPA(미 국방부 최신 기술 연구 프로젝트 관리국)과 일본의 5세대 컴퓨터 프로젝트에 의해서 1980년대 인공지능 연구는 엄청난 연구 기금을 지원 받을 수 있었다. 몇몇 인공지능 선각자들이 거둔 주목할 만한 결과에도 불구하고, 즉각적인 결과를 산출하는 데 실패하게 된다. 이것은 1980년대 후반 인공지능 연구 기금에 대한 대폭적인 삭감을 초래하였고, 인공지능 연구의 침체기를 뜻하는 인공지능의 겨울을 가져왔다. 1990년대, 많은 인공지능 연구가들은 좀 더 구체적인 목적아래기계 학습,로보틱스,컴퓨터 비전과 같은 인공지능과 관련된 하위 영역으로 이동하였고, 순수한 인공지능에 대한 연구는 매우 제한적으로 수행되고 있다.인공지능의 궁극적인 목표인 인간과 같은 지능의 개발이 어려움을 겪자, 다양한 응용 분야가 나타나게 되었다. 대표적인 예가 LISP(리스프)나 Prolog와 같은 언어인데, 애초에 인공지능 연구를 위해 만들어졌으나 지금에 와서는 인공지능과 관련이 없는 분야에서도 사용되고 있다.해커문화도 인공지능 연구실에서 만들어졌는데, 이 중에서도 다양한 시기에 매카시, 민스키, Seymour Papert(Logo를 만들었다), 테리 Winograd(SHRDLU를 만든 뒤에 인공지능을 포기했다)와 같은 유명인의 모태가 된MIT 인공지능 연구소가 유명하다.다른 많은 시스템들이 한때 인공지능의 활발한 연구 주제였던 기술들에 바탕을 두고 만들어졌다. 그 예들은 다음과 같다:인공지능 분야와과학 소설분야에서는 인공지능 시스템이 인간 전문가의 판단을 대체하리라는 예상이 계속해서 제기되어 왔다. 오늘날에는 몇몇 공학이나 의약 조제 같은 특정 분야에서 전문가 시스템이 인간 전문가의 판단을 보조하거나 대체하고 있다.어떤 사람들은 현재 알려진 어떤 시스템보다도 지능적이며 복잡한 시스템의 등장을 예견하기도 한다. 이와 같은 가상적인 시스템들을 '비결정적인 인공지능 시스템'의 약자인 atilects(artificially intelligent non-deterministic systems)라고 한다. 이와 같은 시스템이 만들어진다면 그동안 인류에게 문제시되지 않았던 많은 윤리적인 문제들이 발생하게 된다.이에 대한 토론은 시간이 흐름에 따라 '가능성'보다는 '의도'에 점점 초점을 맞추게 되었다. 이러한 초점의 이동은 Hugo de Garis와 Kevin Warwick에 의해 제기된 "Cosmist"(반대말은 "Terran") 논쟁에 의해 이루어졌다. Garis에 따르면 Cosmist란 더욱 지능적인 종족을 인간의 후계종으로 만들어내기위해 노력한다. 이와 같은 논쟁의 발생은 '의도'의 문제가 초기 사색가들인 "against"들에게 많은 영향을 끼쳤음을 암시한다[모호한 표현].흥미로운 윤리적 문제를 제기하는 이슈들은 다음과 같다.다양한 종류의 지능적 프로그램이 있다. 이들 중 몇 가지 예를 들면 다음과 같다.전 세계에는 수많은 인공지능 연구가들이 있다. 여기서는 인공지능 분야에 많은 기여를 한 연구자들을 소개한다.몇몇 컴퓨터 과학 연구가들은, "인공지능"이라는 용어가 지금까지 이 연구 분야에서 이룩한 많은 업적과 "지능"이라는 일반적인 용어사이에서 큰 불일치를 초래하기 때문에 좋지 못한 용어라고 여겨진다. 이 같은 문제는 대중과학작가들과 Kevin Warwick과 같이 현 상태로는 불가능한 혁신적인 인공지능 연구 성과에 대한 기대를 불러일으키는 사람들에 의해서 심화되고 있다. 이 같은 까닭으로 인공지능과 관련된 분야에서 일하는 많은 연구자들이 자신들은, 인지 과학, 정보학, 통계추론 또는 정보공학과 관련된 연구를 하고 있다고 이야기한다. 그러나 현재 진보는 이루어지고 있고, 오늘날 인공지능은 전 세계 수많은 산업 시스템에서 작동하고 있다. 오늘날 실세계의 인공지능 시스템에 관해 더 자세한 내용을 보려면 Wired지의 기사[8]를 참고하라.